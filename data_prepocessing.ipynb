{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f45cc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the JSON file\n",
    "# with open('data/telegram_full/result.json', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Extract just the 'messages' list\n",
    "# messages = data[\"messages\"]\n",
    "\n",
    "# # Load into a DataFrame\n",
    "# df = pd.DataFrame(messages)\n",
    "\n",
    "# df = df[df[\"action\"].isna()]\n",
    "\n",
    "# print(df[\"text\"].dtypes)\n",
    "\n",
    "# # df.to_csv(\"data/messages.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6eb47d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files='data/messages.csv')\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "03088b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bab6c4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_length(dataset, sample_size=None):\n",
    "    \"\"\"\n",
    "    Determines the max tokenized length in the dataset.\n",
    "    Optionally, limit to a sample of the dataset to save time.\n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "    for i, example in enumerate(dataset[\"text\"]):\n",
    "        if example is not None:\n",
    "            tokenized = tokenizer.tokenize(example)\n",
    "            lengths.append(len(tokenized))\n",
    "        if sample_size and i >= sample_size:\n",
    "            break\n",
    "    return max(lengths)\n",
    "\n",
    "max_len = get_max_length(dataset=split_dataset[\"train\"])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "330e66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    pre_texts = [text if text is not None else \"\" for text in examples[\"text\"]]\n",
    "    texts = []\n",
    "    for i, text in enumerate(pre_texts):\n",
    "        if text != \"\":\n",
    "            texts.append(text)\n",
    "    # print(examples[\"text\"])\n",
    "    # print(\" \".join([x for x in examples[\"text\"] if x is not None]) if examples[\"text\"] else \"\")\n",
    "    # print(tokenizer(str(examples[\"text\"]) if examples[\"text\"] is not None else \"\"))\n",
    "    return tokenizer(texts, max_length=128, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eb3b4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = split_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    num_proc=4,\n",
    "    remove_columns=split_dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2b30c5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "total_empty = 0\n",
    "for sample in tokenized_dataset[\"test\"]:\n",
    "    summa = sum(sample[\"input_ids\"])\n",
    "    if summa == 203:\n",
    "        print(sample)\n",
    "        total_empty += 1\n",
    "\n",
    "print(total_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "42b441f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 4068\n",
      "Test dataset length: 1013\n",
      "{'input_ids': [101, 50190, 6582, 1469, 3124, 34722, 81666, 17942, 9798, 166, 122, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "['[CLS]', 'Че', 'народ', 'на', 'оп', '##пен', '##геймера', 'собрал', 'тогда', '?', ')', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset length: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Test dataset length: {len(tokenized_dataset['test'])}\")\n",
    "\n",
    "print(tokenized_dataset[\"test\"][5])\n",
    "tokens = tokenized_dataset[\"test\"][5][\"input_ids\"]\n",
    "\n",
    "original_text = tokenizer.convert_ids_to_tokens(tokens)\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "79d9ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb572df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = tokenized_dataset.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "99fb0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "11b89735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"DeepPavlov/rubert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8ecdf277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10448/1171318378.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2034' max='2034' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2034/2034 05:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.323300</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.910800</td>\n",
       "      <td>3.692928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.398500</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2034, training_loss=3.8079587923273106, metrics={'train_runtime': 323.0677, 'train_samples_per_second': 37.775, 'train_steps_per_second': 6.296, 'total_flos': 803872285977600.0, 'train_loss': 3.8079587923273106, 'epoch': 3.0})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"RuBertMLM\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.001,\n",
    "    push_to_hub=False,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e6b3db6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='169' max='169' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [169/169 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 40.16\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "55eff2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.44067850708961487,\n",
       "  'token': 1758,\n",
       "  'token_str': 'за',\n",
       "  'sequence': 'Это за спам?'},\n",
       " {'score': 0.12265335023403168,\n",
       "  'token': 3629,\n",
       "  'token_str': 'же',\n",
       "  'sequence': 'Это же спам?'},\n",
       " {'score': 0.07217394560575485,\n",
       "  'token': 11089,\n",
       "  'token_str': 'ли',\n",
       "  'sequence': 'Это ли спам?'},\n",
       " {'score': 0.03132351115345955,\n",
       "  'token': 845,\n",
       "  'token_str': 'в',\n",
       "  'sequence': 'Это в спам?'},\n",
       " {'score': 0.026461344212293625,\n",
       "  'token': 2739,\n",
       "  'token_str': 'как',\n",
       "  'sequence': 'Это как спам?'},\n",
       " {'score': 0.024310747161507607,\n",
       "  'token': 1699,\n",
       "  'token_str': 'не',\n",
       "  'sequence': 'Это не спам?'},\n",
       " {'score': 0.018983563408255577,\n",
       "  'token': 3474,\n",
       "  'token_str': 'или',\n",
       "  'sequence': 'Это или спам?'},\n",
       " {'score': 0.012777060270309448,\n",
       "  'token': 1469,\n",
       "  'token_str': 'на',\n",
       "  'sequence': 'Это на спам?'},\n",
       " {'score': 0.012134999968111515,\n",
       "  'token': 5806,\n",
       "  'token_str': 'через',\n",
       "  'sequence': 'Это через спам?'},\n",
       " {'score': 0.011625571176409721,\n",
       "  'token': 2067,\n",
       "  'token_str': 'был',\n",
       "  'sequence': 'Это был спам?'}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = \"Это [MASK] спам?\"\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", \"/home/sergei/Documents/pets/spambot/RuBertMLM/checkpoint-2034\")\n",
    "mask_filler(text, top_k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spambot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
